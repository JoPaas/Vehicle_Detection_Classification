{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from darkflow.net.build import TFNet\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import YOLO\n",
    "\n",
    "I am using a neural network called YOLO (you only look once), wich is amongst the state-of-the-art CNN's for object detection and classification. In contrast to handcrafted algorithms that detect shapes and features, CNN's can generalize better when trained on a sufficient amount of data, because they figure out the best features to detect and classify objects on their own through back propagation.\n",
    "By loading the weights of the network I don't have to train it myself and can profit from the training that has been done on a large database. It would be possible to retrain the last layers for specific tasks but in this case its not necessary, because it already detects the classes I am looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing ./cfg/yolo.cfg\n",
      "Parsing cfg/yolo.cfg\n",
      "Loading bin/yolo.weights ...\n",
      "Successfully identified 203934260 bytes\n",
      "Finished in 0.020043373107910156s\n",
      "Model has a coco model name, loading coco labels.\n",
      "\n",
      "Building net ...\n",
      "Source | Train? | Layer description                | Output size\n",
      "-------+--------+----------------------------------+---------------\n",
      "       |        | input                            | (?, 608, 608, 3)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 608, 608, 32)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 304, 304, 32)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 304, 304, 64)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 152, 152, 64)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 152, 152, 128)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 76, 76, 128)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 76, 76, 256)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 256)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | maxp 2x2p0_2                     | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 19, 19, 512)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | concat [16]                      | (?, 38, 38, 512)\n",
      " Load  |  Yep!  | conv 1x1p0_1  +bnorm  leaky      | (?, 38, 38, 64)\n",
      " Load  |  Yep!  | local flatten 2x2                | (?, 19, 19, 256)\n",
      " Load  |  Yep!  | concat [27, 24]                  | (?, 19, 19, 1280)\n",
      " Load  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 19, 19, 1024)\n",
      " Load  |  Yep!  | conv 1x1p0_1    linear           | (?, 19, 19, 425)\n",
      "-------+--------+----------------------------------+---------------\n",
      "GPU mode with 0.8 usage\n",
      "Finished in 6.928057909011841s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'model': 'cfg/yolo.cfg',\n",
    "    'load': 'bin/yolo.weights',\n",
    "    'threshold': 0.3,\n",
    "    'gpu': 0.8\n",
    "}\n",
    "\n",
    "tfnet = TFNet(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frame():\n",
    "    def __init__(self):\n",
    "        self.input_img = [np.array([False])]\n",
    "        self.result = [np.array([False])]\n",
    "        self.detections = None\n",
    "        self.output_img = [np.array([False])]\n",
    "\n",
    "# function to filter the detections to class \"car\" only and obtain the bounding boxes in format for drawing them on the image\n",
    "def get_YOLO_detections(frame):\n",
    "    if frame.result:\n",
    "        detections = []\n",
    "        for entry in frame.result:\n",
    "            if entry['label'] == 'car':\n",
    "                detection = [entry['topleft']['x'], entry['topleft']['y'], entry['bottomright']['x'], entry['bottomright']['y'], entry['confidence']]\n",
    "                detections.append(detection)\n",
    "    else:\n",
    "        detections = []\n",
    "    frame.detections = np.array(detections)\n",
    "\n",
    "# function to draw the bounding boxes on the image\n",
    "def draw_detections(frame, thick=6):\n",
    "    draw_img = np.copy(frame.input_img)\n",
    "    #[cv2.rectangle(draw_img, (box[0][0], box[0][1]), (box[1][0], box[1][1]), color, thick) for box in frame.boxes]\n",
    "    colors = ((0, 255, 255), (0, 100, 255), (0, 0, 255), (100, 0, 255), (255, 0, 255), (255, 0, 100), (255, 0, 0), (255, 100, 0), (255, 255, 0), (100, 255, 0), (0, 255, 0))\n",
    "    #print(int(frame.detections_filt[0,-1]%len(colors)))\n",
    "    [cv2.rectangle(draw_img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), colors[int(box[-1]%len(colors))], thick) for box in frame.detections_filt]\n",
    "    #[cv2.circle(draw_img, center, 10, color, -1) for center in frame.centers]\n",
    "    frame.output_img = draw_img\n",
    "\n",
    "def update_detection_filter(frame, tracker):\n",
    "    #print(frame.detections)\n",
    "    frame.detections_filt = tracker.update(frame.detections)\n",
    "    #print(frame.detections_filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracker\n",
    "(imported from github repo)\n",
    "I am using a kalman filter for each individual detection to estimate the detection position in the next frame. In the update step the kalman gain decides the weights of prediction and measurement to update the estimate.\n",
    "\n",
    "The state contains:\n",
    "\n",
    "* center point x\n",
    "* center point y\n",
    "* area size (w\\*h)\n",
    "* aspect ratio (w/h)\n",
    "* velocity x\n",
    "* velocity y\n",
    "\n",
    "And is calculated for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    SORT: A Simple, Online and Realtime Tracker\n",
    "    Copyright (C) 2016 Alex Bewley alex@dynamicdetection.com\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\"\"\"\n",
    "\n",
    "@jit\n",
    "def iou(bb_test,bb_gt):\n",
    "    \"\"\"\n",
    "    Computes IUO between two bboxes in the form [x1,y1,x2,y2]\n",
    "    \"\"\"\n",
    "    xx1 = np.maximum(bb_test[0], bb_gt[0])\n",
    "    yy1 = np.maximum(bb_test[1], bb_gt[1])\n",
    "    xx2 = np.minimum(bb_test[2], bb_gt[2])\n",
    "    yy2 = np.minimum(bb_test[3], bb_gt[3])\n",
    "    w = np.maximum(0., xx2 - xx1)\n",
    "    h = np.maximum(0., yy2 - yy1)\n",
    "    wh = w * h\n",
    "    o = wh / ((bb_test[2]-bb_test[0])*(bb_test[3]-bb_test[1]) + (bb_gt[2]-bb_gt[0])*(bb_gt[3]-bb_gt[1]) - wh)\n",
    "    return(o)\n",
    "\n",
    "def convert_bbox_to_z(bbox):\n",
    "    \"\"\"\n",
    "    Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form\n",
    "    [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is\n",
    "    the aspect ratio\n",
    "    \"\"\"\n",
    "    w = bbox[2]-bbox[0]\n",
    "    h = bbox[3]-bbox[1]\n",
    "    x = bbox[0]+w/2.\n",
    "    y = bbox[1]+h/2.\n",
    "    s = w*h    #scale is just area\n",
    "    r = w/float(h)\n",
    "    return np.array([x,y,s,r]).reshape((4,1))\n",
    "\n",
    "def convert_x_to_bbox(x,score=None):\n",
    "    \"\"\"\n",
    "    Takes a bounding box in the centre form [x,y,s,r] and returns it in the form\n",
    "    [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right\n",
    "    \"\"\"\n",
    "    w = np.sqrt(x[2]*x[3])\n",
    "    h = x[2]/w\n",
    "    if(score==None):\n",
    "        return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.]).reshape((1,4))\n",
    "    else:\n",
    "        return np.array([x[0]-w/2.,x[1]-h/2.,x[0]+w/2.,x[1]+h/2.,score]).reshape((1,5))\n",
    "\n",
    "\n",
    "class KalmanBoxTracker(object):\n",
    "    \"\"\"\n",
    "    This class represents the internel state of individual tracked objects observed as bbox.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    def __init__(self,bbox):\n",
    "        \"\"\"\n",
    "        Initialises a tracker using initial bounding box.\n",
    "        \"\"\"\n",
    "        #define constant velocity model\n",
    "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "        self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\n",
    "        self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\n",
    "    \n",
    "        self.kf.R[2:,2:] *= 10.\n",
    "        self.kf.P[4:,4:] *= 1000. #give high uncertainty to the unobservable initial velocities\n",
    "        self.kf.P *= 10.\n",
    "        self.kf.Q[-1,-1] *= 0.01\n",
    "        self.kf.Q[4:,4:] *= 0.01\n",
    "    \n",
    "        self.kf.x[:4] = convert_bbox_to_z(bbox)\n",
    "        self.time_since_update = 0\n",
    "        self.id = KalmanBoxTracker.count\n",
    "        KalmanBoxTracker.count += 1\n",
    "        self.history = []\n",
    "        self.hits = 0\n",
    "        self.hit_streak = 0\n",
    "        self.age = 0\n",
    "\n",
    "    def update(self,bbox):\n",
    "        \"\"\"\n",
    "        Updates the state vector with observed bbox.\n",
    "        \"\"\"\n",
    "        self.time_since_update = 0\n",
    "        self.history = []\n",
    "        self.hits += 1\n",
    "        self.hit_streak += 1\n",
    "        self.kf.update(convert_bbox_to_z(bbox))\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Advances the state vector and returns the predicted bounding box estimate.\n",
    "        \"\"\"\n",
    "        if((self.kf.x[6]+self.kf.x[2])<=0):\n",
    "            self.kf.x[6] *= 0.0\n",
    "        self.kf.predict()\n",
    "        self.age += 1\n",
    "        if(self.time_since_update>0):\n",
    "            self.hit_streak = 0\n",
    "        self.time_since_update += 1\n",
    "        self.history.append(convert_x_to_bbox(self.kf.x))\n",
    "        return self.history[-1]\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Returns the current bounding box estimate.\n",
    "        \"\"\"\n",
    "        return convert_x_to_bbox(self.kf.x)\n",
    "\n",
    "def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):\n",
    "    \"\"\"\n",
    "    Assigns detections to tracked object (both represented as bounding boxes)\n",
    "\n",
    "    Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n",
    "    \"\"\"\n",
    "    if(len(trackers)==0):\n",
    "        return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
    "    iou_matrix = np.zeros((len(detections),len(trackers)),dtype=np.float32)\n",
    "\n",
    "    for d,det in enumerate(detections):\n",
    "        for t,trk in enumerate(trackers):\n",
    "            iou_matrix[d,t] = iou(det,trk)\n",
    "    matched_indices = linear_assignment(-iou_matrix)\n",
    "\n",
    "    unmatched_detections = []\n",
    "    for d,det in enumerate(detections):\n",
    "        if(d not in matched_indices[:,0]):\n",
    "            unmatched_detections.append(d)\n",
    "    unmatched_trackers = []\n",
    "    for t,trk in enumerate(trackers):\n",
    "        if(t not in matched_indices[:,1]):\n",
    "            unmatched_trackers.append(t)\n",
    "\n",
    "    #filter out matched with low IOU\n",
    "    matches = []\n",
    "    for m in matched_indices:\n",
    "        if(iou_matrix[m[0],m[1]]<iou_threshold):\n",
    "            unmatched_detections.append(m[0])\n",
    "            unmatched_trackers.append(m[1])\n",
    "        else:\n",
    "            matches.append(m.reshape(1,2))\n",
    "    if(len(matches)==0):\n",
    "        matches = np.empty((0,2),dtype=int)\n",
    "    else:\n",
    "        matches = np.concatenate(matches,axis=0)\n",
    "\n",
    "    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
    "\n",
    "\n",
    "\n",
    "class Sort(object):\n",
    "    def __init__(self,max_age=1,min_hits=3):\n",
    "        \"\"\"\n",
    "        Sets key parameters for SORT\n",
    "        \"\"\"\n",
    "        self.max_age = max_age\n",
    "        self.min_hits = min_hits\n",
    "        self.trackers = []\n",
    "        self.frame_count = 0\n",
    "\n",
    "    def update(self,dets):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]\n",
    "        Requires: this method must be called once for each frame even with empty detections.\n",
    "        Returns the a similar array, where the last column is the object ID.\n",
    "\n",
    "        NOTE: The number of objects returned may differ from the number of detections provided.\n",
    "        \"\"\"\n",
    "        self.frame_count += 1\n",
    "        #get predicted locations from existing trackers.\n",
    "        trks = np.zeros((len(self.trackers),5))\n",
    "        to_del = []\n",
    "        ret = []\n",
    "        for t,trk in enumerate(trks):\n",
    "            pos = self.trackers[t].predict()[0]\n",
    "            trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n",
    "            if(np.any(np.isnan(pos))):\n",
    "                to_del.append(t)\n",
    "        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n",
    "        for t in reversed(to_del):\n",
    "            self.trackers.pop(t)\n",
    "        matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets,trks)\n",
    "\n",
    "        #update matched trackers with assigned detections\n",
    "        for t,trk in enumerate(self.trackers):\n",
    "            if(t not in unmatched_trks):\n",
    "                d = matched[np.where(matched[:,1]==t)[0],0]\n",
    "                trk.update(dets[d,:][0])\n",
    "\n",
    "        #create and initialise new trackers for unmatched detections\n",
    "        for i in unmatched_dets:\n",
    "            trk = KalmanBoxTracker(dets[i,:]) \n",
    "            self.trackers.append(trk)\n",
    "        i = len(self.trackers)\n",
    "        for trk in reversed(self.trackers):\n",
    "            d = trk.get_state()[0]\n",
    "            if((trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits)):\n",
    "                ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n",
    "            i -= 1\n",
    "            #remove dead tracklet\n",
    "            if(trk.time_since_update > self.max_age):\n",
    "                self.trackers.pop(i)\n",
    "        if(len(ret)>0):\n",
    "            return np.concatenate(ret)\n",
    "        return np.empty((0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = frame()\n",
    "tracker = Sort()\n",
    "f.input_img = cv2.imread('test_images/test1.jpg')\n",
    "f.result = tfnet.return_predict(f.input_img)\n",
    "print(f.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_YOLO_detections(f)\n",
    "update_detection_filter(f, tracker)\n",
    "draw_detections(f, thick=6)\n",
    "\n",
    "cv2.imshow(\"image\", f.output_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "f = frame()\n",
    "tracker = Sort(max_age=1, min_hits=3)\n",
    "visualize = True\n",
    "# read video file\n",
    "cap = cv2.VideoCapture(\"project_video.mp4\")\n",
    "# set up video writer\n",
    "shape_img = cap.read()[1].shape[1::-1]\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "out = cv2.VideoWriter(\"output_videos\\\\project_video_1.mp4\",fourcc, 20.0, shape_img)\n",
    "# process video\n",
    "while(cap.isOpened()):\n",
    "    ff,f.input_img = cap.read()\n",
    "    if(ff == True):\n",
    "        # obtain YOLO result\n",
    "        f.result = tfnet.return_predict(f.input_img)\n",
    "        # get detections\n",
    "        get_YOLO_detections(f)\n",
    "        # update filter\n",
    "        update_detection_filter(f, tracker)\n",
    "        # draw bounding boxes\n",
    "        draw_detections(f, thick=6)\n",
    "        # save output\n",
    "        out.write(f.output_img)\n",
    "        # visualize\n",
    "        if(visualize == True):\n",
    "            cv2.imshow('frame',f.output_img)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
